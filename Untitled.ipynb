{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from model_search import Network\n",
    "from architect1 import Architect\n",
    "from resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"cifar\")\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('--data', type=str, default='../data', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=8, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.025, help='init learning rate')\n",
    "parser.add_argument('--learning_rate_min', type=float, default=0.001, help='min learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')\n",
    "parser.add_argument('--report_freq', type=float, default=1, help='report frequency')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--epochs', type=int, default=30, help='num of training epochs')\n",
    "parser.add_argument('--init_channels', type=int, default=16, help='num of init channels')\n",
    "parser.add_argument('--layers', type=int, default=8, help='total number of layers')\n",
    "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
    "parser.add_argument('--cutout', action='store_true', default=False, help='use cutout')\n",
    "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0.3, help='drop path probability')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=2, help='random seed')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "parser.add_argument('--train_portion', type=float, default=0.5, help='portion of training data')\n",
    "parser.add_argument('--unrolled', action='store_true', default=True, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--arch_learning_rate', type=float, default=3e-4, help='learning rate for arch encoding')\n",
    "parser.add_argument('--arch_weight_decay', type=float, default=1e-3, help='weight decay for arch encoding')\n",
    "parser.add_argument('--lambda_par', type=float, default=1.0, help='unlabeled ratio')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.save = 'search-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "\n",
    "CIFAR_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(args.seed)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "logging.info('gpu device = %d' % args.gpu)\n",
    "logging.info(\"args = %s\", args)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()\n",
    "criterion_stud = nn.CrossEntropyLoss()\n",
    "criterion_stud = criterion_stud.cuda()\n",
    "criterion_mid = nn.CrossEntropyLoss()\n",
    "criterion_mid = criterion_stud.cuda()\n",
    "model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion)\n",
    "model = model.cuda()\n",
    "logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\n",
    "student = ResNet(criterion_stud)\n",
    "student = student.cuda()\n",
    "mid = ResNet(criterion_mid)\n",
    "mid = mid.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),args.learning_rate,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "optimizer_stud = torch.optim.SGD(student.parameters(),args.learning_rate,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "optimizer_mid = torch.optim.SGD(mid.parameters(),args.learning_rate,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "\n",
    "train_transform, valid_transform = utils._data_transforms_cifar10(args)\n",
    "train_transform1, valid_transform1 = utils._data_transforms_cifar100(args)\n",
    "train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\n",
    "u_data = dset.CIFAR100(root=args.data, train=True, download=True, transform=train_transform1)\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(args.train_portion * num_train))\n",
    "\n",
    "train_queue = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size,\n",
    "          sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "          pin_memory=True, num_workers=2)\n",
    "\n",
    "valid_queue = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size,\n",
    "          sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "          pin_memory=True, num_workers=2)\n",
    "    \n",
    "unlabeled_queue = torch.utils.data.DataLoader(u_data, batch_size=args.batch_size,\n",
    "          sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:]),\n",
    "          pin_memory=True, num_workers=2)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
    "\n",
    "architect = Architect(model, mid, student, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cusloss(inp, tar):\n",
    "    m = nn.Softmax(1)\n",
    "    lm = nn.LogSoftmax(1)\n",
    "    lenn = inp.shape[0]\n",
    "    inp = lm(inp)\n",
    "    tar = m(tar)\n",
    "    out = inp*tar\n",
    "    ll = (out.sum()*(-1))/lenn\n",
    "    return ll\n",
    "\n",
    "def train(train_queue, valid_queue, unlabeled_queue, model, mid, student, architect, criterion, criterion_mid, criterion_stud, optimizer, optimizer_mid, optimizer_stud, lr):\n",
    "  objs = utils.AvgrageMeter()\n",
    "  top1 = utils.AvgrageMeter()\n",
    "  top5 = utils.AvgrageMeter()\n",
    "\n",
    "  print(\"1---------------------------\")\n",
    "  for step, (input, target) in enumerate(train_queue):\n",
    "    print(\"2---------------------------\")\n",
    "    model.train()\n",
    "    n = input.size(0)\n",
    "    input = input.cuda()\n",
    "    target = target.cuda(non_blocking=True)\n",
    "\n",
    "    # get a random minibatch from the search queue with replacement\n",
    "    try:\n",
    "            input_search, target_search = next(valid_queue_iter)\n",
    "    except:\n",
    "            valid_queue_iter = iter(valid_queue)\n",
    "            input_search, target_search = next(valid_queue_iter)\n",
    "    input_search = input_search.cuda()\n",
    "    target_search = target_search.cuda(non_blocking=True)\n",
    "    \n",
    "    # get a random minibatch from the unlabeled queue with replacement\n",
    "    try:\n",
    "            input_unlabeled, target_unlabeled = next(unlabeled_queue_iter)\n",
    "    except:\n",
    "            unlabeled_queue_iter = iter(unlabeled_queue)\n",
    "            input_unlabeled, target_unlabeled = next(unlabeled_queue_iter)\n",
    "    input_unlabeled = input_unlabeled.cuda()\n",
    "    target_unlabeled = target_unlabeled.cuda(non_blocking=True)\n",
    "    \n",
    "    #print(\"start###############################\")\n",
    "    architect.step_all3(input, target, input_search, target_search, input_unlabeled, lr, optimizer, optimizer_mid, optimizer_stud, unrolled=args.unrolled)\n",
    "    #print(\"end#################################\")\n",
    "    \n",
    "    #print(\"s1---------------------------\")\n",
    "    #architect.step(input, target, input_search, target_search, input_unlabeled, lr, optimizer, unrolled=args.unrolled)\n",
    "    #print(\"s2---------------------------\")\n",
    "    #architect.step1(input, target, input_search, target_search, input_unlabeled, lr, optimizer, optimizer_mid, unrolled=args.unrolled)\n",
    "    #print(\"s3---------------------------\")\n",
    "    #architect.step2(input, target, input_search, target_search, input_unlabeled, lr, optimizer, optimizer_mid, optimizer_stud, unrolled=args.unrolled)\n",
    "    #print(\"s4---------------------------\")\n",
    "    \n",
    "    ##########################################################################################################\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input)\n",
    "    loss = criterion(logits, target)\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##########################################################################################################\n",
    "    \n",
    "    optimizer_mid.zero_grad()\n",
    "    l1 = model(input_unlabeled)\n",
    "    logits1 = mid(input_unlabeled)\n",
    "    loss1 = cusloss(logits1, l1.detach())\n",
    "\n",
    "    #loss1.backward()\n",
    "    #nn.utils.clip_grad_norm(mid.parameters(), args.grad_clip)\n",
    "    #optimizer_mid.step()\n",
    "       \n",
    "    #optimizer_mid.zero_grad()\n",
    "    logits2 = mid(input)\n",
    "    loss2 = criterion_mid(logits2, target)\n",
    "    \n",
    "    loss5 = loss1 + loss2\n",
    "    loss5.backward()\n",
    "    #nn.utils.clip_grad_norm(mid.parameters(), args.grad_clip)\n",
    "    optimizer_mid.step()\n",
    "    \n",
    "    ##########################################################################################################\n",
    "    \n",
    "    optimizer_stud.zero_grad()\n",
    "    l3 = mid(input_unlabeled)\n",
    "    logits3 = student(input_unlabeled)\n",
    "    loss3 = cusloss(logits3, l3.detach())\n",
    "    \n",
    "    #loss3.backward()\n",
    "    #nn.utils.clip_grad_norm(student.parameters(), args.grad_clip)\n",
    "    #optimizer_stud.step()\n",
    "       \n",
    "    #optimizer_stud.zero_grad()\n",
    "    logits4 = student(input)\n",
    "    loss4 = criterion_stud(logits4, target)\n",
    "\n",
    "    loss6 = loss3 + loss4\n",
    "    loss6.backward()\n",
    "    #nn.utils.clip_grad_norm(student.parameters(), args.grad_clip)\n",
    "    optimizer_stud.step()\n",
    "    \n",
    "    ##########################################################################################################\n",
    "\n",
    "    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "    objs.update(loss.item(), n)\n",
    "    top1.update(prec1.item(), n)\n",
    "    top5.update(prec5.item(), n)\n",
    "\n",
    "    if step % args.report_freq == 0:\n",
    "      logging.info('train %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "\n",
    "\n",
    "  return top1.avg, objs.avg\n",
    "\n",
    "def infer(valid_queue, model, criterion):\n",
    "  objs = utils.AvgrageMeter()\n",
    "  top1 = utils.AvgrageMeter()\n",
    "  top5 = utils.AvgrageMeter()\n",
    "  model.eval()\n",
    "\n",
    "  for step, (input, target) in enumerate(valid_queue):\n",
    "    input = Variable(input, volatile=True).cuda()\n",
    "    target = Variable(target, volatile=True).cuda()\n",
    "\n",
    "    logits = model(input)\n",
    "    loss = criterion(logits, target)\n",
    "\n",
    "    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "    n = input.size(0)\n",
    "    objs.update(loss.item(), n)\n",
    "    top1.update(prec1.item(), n)\n",
    "    top5.update(prec5.item(), n)\n",
    "\n",
    "    if step % args.report_freq == 0:\n",
    "      logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "    \n",
    "\n",
    "  return top1.avg, objs.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(args.epochs):\n",
    "        scheduler.step()\n",
    "        lr = scheduler.get_lr()[0]\n",
    "        logging.info('epoch %d lr %e', epoch, lr)\n",
    "\n",
    "        genotype = model.genotype()\n",
    "        logging.info('genotype = %s', genotype)\n",
    "\n",
    "        print(F.softmax(model.alphas_normal, dim=-1))\n",
    "        print(F.softmax(model.alphas_reduce, dim=-1))\n",
    "        print(\"---------------------------\")\n",
    "\n",
    "        # training\n",
    "        train_acc, train_obj = train(train_queue, valid_queue, unlabeled_queue, model, mid, student, architect, criterion, criterion_mid, criterion_stud, optimizer, optimizer_mid, optimizer_stud, lr)\n",
    "        logging.info('train_acc %f', train_acc)\n",
    "\n",
    "        # validation\n",
    "        valid_acc, valid_obj = infer(valid_queue, model, criterion)\n",
    "        logging.info('valid_acc %f', valid_acc)\n",
    "\n",
    "        utils.save(model, os.path.join(args.save, 'weights.pt'))\n",
    "genotype = model.genotype()\n",
    "logging.info('genotype = %s', genotype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
